# ðŸ§± FAZA 1 â€“ Preprocesare date & Feature Engineering
ðŸ“… **DuratÄƒ estimatÄƒ:** 2 sÄƒptÄƒmÃ¢ni  
ðŸŽ¯ **Output final:** dataset curat, pregÄƒtit pentru antrenarea modelelor ML

---

## âš™ï¸ 1ï¸âƒ£ Obiectivul principal

PregÄƒtirea datelor brute (ex. din [Kaggle â€“ Marketing Campaign Dataset](https://www.kaggle.com/datasets/rodsaldanha/arketing-campaign)) pentru modele de:
- **Clasificare** â†’ prezicerea succesului (`Response`)
- **Regresie** â†’ estimarea ROI (`ROI`)
- **Clustering** â†’ segmentarea audienÈ›ei (`KMeans`)

---

## ðŸ§© 2ï¸âƒ£ Structura fiÈ™ierelor implicate

**Input:**  
`data/raw/marketing_campaign.csv` (fiÈ™ier original Kaggle)

**Output:**  
`data/processed/marketing_campaign_clean.csv`  
`data/processed/features_selected.csv`

---

## ðŸ§¹ 3ï¸âƒ£ Etapele principale de preprocesare

### ðŸ”¸ a) Import È™i analizÄƒ iniÈ›ialÄƒ

```python
import pandas as pd

df = pd.read_csv("data/raw/marketing_campaign.csv")
print(df.info())
print(df.describe())
print(df.isna().sum())
```

âž¡ï¸ Scop: Ã®nÈ›elegerea structurii datelor, tipurile de coloane, valorile lipsÄƒ È™i distribuÈ›iile.

---

### ðŸ”¸ b) CurÄƒÈ›area datelor

1. **EliminÄƒ valorile lipsÄƒ (NaN):**
```python
df = df.dropna(subset=["Income", "Education", "Marital_Status"])
```

2. **EliminÄƒ duplicatele:**
```python
df = df.drop_duplicates()
```

3. **CorecteazÄƒ formatele:**
```python
df["Dt_Customer"] = pd.to_datetime(df["Dt_Customer"], errors="coerce")
df["Year_Birth"] = df["Year_Birth"].astype(int)
```

4. **FiltreazÄƒ vÃ¢rste aberante:**
```python
df = df[(df["Year_Birth"] > 1940) & (df["Year_Birth"] < 2005)]
```

---

### ðŸ”¸ c) Crearea de coloane derivate (Feature Engineering)

1. **CalculeazÄƒ vÃ¢rsta clientului:**
```python
from datetime import datetime
df["Age"] = datetime.now().year - df["Year_Birth"]
```

2. **NumÄƒr total de cumpÄƒrÄƒturi:**
```python
df["TotalPurchases"] = df["NumDealsPurchases"] + df["NumWebPurchases"] + df["NumCatalogPurchases"] + df["NumStorePurchases"]
```

3. **Cheltuieli totale:**
```python
df["TotalSpent"] = df[["MntWines", "MntFruits", "MntGoldProds", "MntMeatProducts", "MntSweetProducts", "MntFishProducts"]].sum(axis=1)
```

4. **Categorii de vÃ¢rstÄƒ:**
```python
df["AgeGroup"] = pd.cut(df["Age"], bins=[18, 30, 45, 60, 80], labels=["Young", "Adult", "Mature", "Senior"])
```

5. **Timp de la ultima cumpÄƒrare (Recency bucket):**
```python
df["RecencyGroup"] = pd.cut(df["Recency"], bins=[0, 30, 60, 120, 365], labels=["Recent", "Mid", "Old", "Dormant"])
```

---

### ðŸ”¸ d) CurÄƒÈ›are categorii È™i encoding

1. **Normalizare text:**
```python
df["Education"] = df["Education"].str.strip().replace({"PhD": "Doctor", "2n Cycle": "Graduate"})
df["Marital_Status"] = df["Marital_Status"].str.title()
```

2. **Encoding pentru variabile categorice:**
```python
df = pd.get_dummies(df, columns=["Education", "Marital_Status", "AgeGroup", "RecencyGroup"], drop_first=True)
```

---

### ðŸ”¸ e) Feature scaling (standardizare numericÄƒ)

Pentru modele bazate pe distanÈ›Äƒ (KMeans, regresie liniarÄƒ etc.):

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
num_cols = ["Income", "TotalPurchases", "TotalSpent", "Recency", "Age"]
df[num_cols] = scaler.fit_transform(df[num_cols])
```

SalveazÄƒ scalerul pentru reutilizare:
```python
import joblib
joblib.dump(scaler, "data/models/scaler.joblib")
```

---

### ðŸ”¸ f) Salvare dataset curat

```python
df.to_csv("data/processed/marketing_campaign_clean.csv", index=False)
```

---

## ðŸ“ˆ 4ï¸âƒ£ Validare È™i verificare calitate date

| Verificare | Scop | Exemplu de cod |
|-------------|------|----------------|
| Lipsa valorilor nule | AsigurÄƒ completitudinea datelor | `df.isna().sum()` |
| DistribuÈ›ia numericÄƒ | DetecteazÄƒ anomalii | `df[num_cols].describe()` |
| CorelaÈ›ii | IdentificÄƒ relaÈ›ii utile | `df.corr()` |
| Dimensiunea finalÄƒ | ConfirmÄƒ consistenÈ›a | `df.shape` |

---

## ðŸ§  5ï¸âƒ£ Rezultate finale

DupÄƒ faza 1, obÈ›ii:
- âœ… date curate, fÄƒrÄƒ valori lipsÄƒ/aberante,  
- âœ… coloane derivate relevante (`TotalSpent`, `TotalPurchases`, `AgeGroup` etc.),  
- âœ… variabile normalizate,  
- âœ… set salvat Ã®n `data/processed/`.

---

## ðŸ’¡ 6ï¸âƒ£ (OpÈ›ional) Notebook pentru faza 1

CreeazÄƒ un Jupyter notebook Ã®n `notebooks/EDA.ipynb` cu etapele de mai sus, incluzÃ¢nd:
- histogramÄƒ pentru distribuÈ›ia vÃ¢rstei,  
- grafic pentru cheltuieli totale,  
- heatmap pentru corelaÈ›ii (`seaborn.heatmap(df.corr())`).

---

ðŸ“„ **Rezultat livrabil:** `docs/Faza1_Preprocesare_Date.md` + `data/processed/marketing_campaign_clean.csv`
